\section{Markov Chains}

A Markov chain is a random sequence of values from some space. In contrast to any other random sequence, a Markov chain is characterized the particular property that the distribution determining every value in the chain is dependent on exactly the directly preceding value. If considered as a series of steps through some space of values, this means that every step taken is only based on the current location in the space, and entirely agnostic to how we got there. Or perhaps more philosophically, in a Markov chain, the future depends on the present, but not the past.

In mathematical terms, we define a \textit{Markov chain} as a series $(x_t)_{t \in \mathbb{N}_0}$ in some space $\mathbb{X}$, where we choose the starting value $x_0$, and every value $x_{t + 1}$ after that is defined in accordance to some \textit{transition kernel} $k$, $x_{t+1} \sim k[x_t]$.

The advantage of such a forgetful sequence is that it is possible to prove general propositions about it's behavior, such as whether or not it will converge towards a stable distribution and what this distribution will look like. But by each value depending on the previous, much more interesting behavior can emerge than with a sequence of entirely independently drawn samples.

While Markov chains are often times conceptualized as a directed graph consisting of nodes representing the possible states the chain can jump between at each step, and weighted edges representing possible transitions between states with their relative probability, for our purposes it is more useful to consider a Markov chain as a series of time-discrete steps in some, potentially high-dimensional and highly structured, abstract space of values.

At each point in time therefore we are at some value and make a decision as to which value to jump to next. The choices of these incremental decisions will accumulate to determine the properties of the resulting sequence of values.

As a tangible example, one might consider the one dimensional array of integer numbers, where at each step we throw a coin to determine whether to take a step in the negative direction or a step in the positive direction. Or for another example perhaps a two dimensional space of real-numbered pairs and a walk throughout it where at each step a value from a two dimensional normal distribution is draw to determine in what direction and how far to jump next. In either case, the result will be a random sequence of values characterized by both the space they came from and the random distribution by which we stepped through this space.

The core property of Markov chains that is of interest for our purposes is that, if the transition kernel satisfies certain properties, the series $(x_i)_{i \in \mathbb{N}}$ will stochastically converge to a certain \textit{equilibrium distribution} $p_eq$ \cite{bonawitz2008composable}, meaning that the series will for higher and higher $t$ tend to look more and more as if each value $x_t$ was simply drawn from $p_eq$.
