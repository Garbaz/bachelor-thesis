\section{Metropolis Hastings}

While there are many Markov chain Monte Carlo algorithms (MCMC), with differing conditions for application and advantages/disadvantages in terms of implementation complexity and performance, we will focus here on a principally rather simple algorithm, the "Metropolis-Hastings algorithm" (MH) \cite{metropolis1953equation} \cite{hastings1970monte}.

The problem we are trying to solve with MH is the desire to obtain representative samples from some distribution of interest $\pi$, the \textit{target}. And as is the principle of MCMC methods, we do so by exploring the $\mathbb{X}$ of possible samples from $\pi$, it's \textit{support}, via the iterative development of a Markov chain through this space. How exactly these steps are taken is the principle concern of MH.

To apply MH to sampling from some distribution $\pi$, we first need to pick a (usually, relative to $\pi$, very simple) distribution $q[x]$, the \textit{proposal kernel}, with the same support as $\pi$. This kernel defines our exploration of the support $\mathbb{X}$. Specifically, at every step $x_t$ in our Markov chain we use $q$ to choose a possible next step $\hat{x}_{t+1} \sim q[x_t]$. So $q$ can, and usually will, depend on the current position $x_t$ in $\mathbb{X}$.

If our $\pi$ were for example defined over $\mathbb{X} = \mathbb{R}^2$, a possible candidate for $q$ would be a two-dimensional normal distribution centered around $x_t$. So at each step in Markov chain exploration of $\mathbb{X}$, we would draw $\hat{x}_{t+1} \sim \mathcal{N}^2[\mu = x_t, \sigma^2 = s^2]$ (with $s^2$ being a parameter to be tuned for our Markov chain to converge faster).

If we were to simply always take this proposed step $\hat{x}_{t+1}$ drawn from $q[x_t]$, then the result would be a random walk through $\mathbb{X}$ entirely independent from our target distribution $\pi$. With the goal of course being to obtain a series of samples from $\pi$, this would of course not be of much use.

The second part to every sampling step in the MH algorithm is to decide whether to take the proposed step drawn from $q[x_t]$, $x_{t+1} := \hat{x}_{t+1}$, or whether to remain at the current position in $\mathbb{X}$, $x_{t+1} := x_t$. This decision is itself done randomly:

\begin{align*}
x_{t+1} & = \begin{cases}
    \hat{x}_{t+1} & \text{ if } u \le \alpha \\
            x_{t} & \text{ otherwise }
            \end{cases} \\
\alpha & = \frac{\pi(\hat{x}_{t+1})}{\pi(x_t)} \frac{q(x_t | \hat{x}_{t+1})}{q(\hat{x}_{t+1} | x_t)} \\
u & \sim \mathcal{U}[0,1]
\end{align*}

The value $\alpha$ defined here is called the \textit{acceptance ratio}. If $\alpha$ is equal to $0$, say if $\pi(\hat{x}_{t+1}) = 0$, then we will definitely remain in place, since $P(U \le 0) = 0$. So our Markov chain will never step to values which are impossible to be a sample from $\pi$.

If on the other hand $\alpha$ is greater than or equal to $1$, say if $\pi(\hat{x}_{t+1}) \ge {\pi(x_t)}$ and $q(x_t | \hat{x}_{t+1}) = q(\hat{x}_{t+1} | x_t)$, then we will definitely take the proposed step, since $P(U \le 1) = 1$. So our Markov chain will always tend towards regions of more likely values under $\pi$.

For any $\alpha$ between $0$ and $1$, we will sometimes take the proposed step and sometimes will not, depending on what value is drawn for $U$. So we can still step "back down" to values that are less likely than the current value, but this is increasingly unlikely the smaller the ratio between the probability under $\pi$ of the value after a proposed step and the current value. As a result, we will generally tend towards sampling from regions of high probability under $\pi$, while also in the long run exploring regions of lower (positive) probability.

And under some assumptions about the target distribution $\pi$ and the kernel $q$ it is possible to rigorously prove that, at least in the limit, the Markov chain of samples generated as such will eventually converge to being a sequence of (dependent) samples from $\pi$ \cite{metropolis1953equation}.

So in total the complete Metropolis-Hastings algorithm looks as follows:

\begin{minipage}{\linewidth}
\begin{itemize}
\item Repeat forever:
  \begin{itemize}
  \item Sample $\hat{x}_{t+1}$ from $q(x_t)$
  \item Calculate acceptance ratio $\alpha$ based upon $\hat{x}_{t+1}$ and $x_t$
  \item Sample $u \sim \mathcal{U}[0,1]$
  \item If $u \le \alpha$, then $x_{t+1} := \hat{x}_{t+1}$, else $x_{t+1} := x_t$
  \end{itemize}
\end{itemize}
\end{minipage}

One in practice highly relevant property to note about the definition of MH is the fact that we only ever need to compute a ratio $\frac{\pi(x)}{\pi(y)}$ between two results of the probability density function $\pi(x)$. This means that we do not actually have to be able to compute $\pi(x)$ directly, but rather that it is sufficient to be able to compute some proportional function $\tilde{\pi}(x) \propto \pi(x)$, since $\frac{\tilde{\pi}(x)}{\tilde{\pi}(y)} = \frac{\pi(x)}{\pi(y)}$.

As a very common practical example, say we would like to generate samples from some posterior distribution:

\begin{equation*}
\pi(x) = p(x | w) = \frac{p(w | x) p(x)}{p(w)}
\end{equation*}

While calculating $p(w | x)$ and $p(x)$ might be straightforward, often times directly getting a value for $p(w)$ is rather difficult. Usually one would have to compute it from the other two quantities as $p(w) = \int p(w | x) p(x) dx$.

With MH however, since $p(w)$ does not depend on $x$ and we only need to know $\pi(x)$ up to a proportionality constant, we can simply define $\tilde{\pi}(x) = p(w) \pi(x) = p(w | x) p(x)$ and calculate our acceptance ratio $\alpha$ with respect to $\tilde{\pi}$ rather than $\pi$, sidestepping the need to evaluate any possibly intractable integrals.
